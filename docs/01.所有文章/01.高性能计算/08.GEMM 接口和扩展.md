---
title: GEMM 接口和扩展
date: 2024-08-17 21:03:51
permalink: /pages/d787b3/
categories:
  - 所有文章
  - 高性能计算
---

## 1. GEMM

BLAS (Basic Linear Algebra Subprograms) 是线性代数接口的规范。

GEMM（General Matrix to Matrix Multiplication，通用矩阵乘）是 BLAS 的一部分，核心就是将两个矩阵相乘。更准确地，是给定矩阵 $A, B, C$ 和数字 $\alpha, \beta$，计算：

$$C \leftarrow \alpha A B + \beta C$$

计算的结果会把 C 的内存覆盖。

GEMM 对 $\beta=0$ 有特殊处理，公式为 $C \leftarrow \alpha A B$，即使 C 有浮点数吉祥三宝（即 inf, -inf, nan）也能正常运算。因此 C **可以**是一块未初始化的内存。

## 2. 参数

BLAS 有两套接口，CBLAS 和 BLAS (FORTRAN)。因为 torch 使用了 FORTRAN 接口，这里就按 FORTRAN 接口介绍。

GEMM 有 13 个参数：transa, transb, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc，含义如下：

- transa, transb: 字符类型，控制矩阵 A, B 转置（字符 N 表示不转置 NO TRANSPOSE，T 表示转置，另外还支持 Hermitian 伴随，这个我们不关心）
- m: 整数，A 的第一维
- n: 整数，B 的第二维
- k: 整数，公共维（A 的第二维、B 的第一维）
- alpha, beta: 浮点数，即公式里的两个数字
- a, b, c: 指针，指向矩阵 A, B, C 首地址（首行首列元素）
- lda, ldb, ldc: 整数，是 A, B, C 矩阵的 stride，换个说法就是用 `a[i + j * lda]` 访问矩阵 A 的 i 行 j 列

## 3. 行列优先顺序

这时候有人就会发现盲点，`a[i + j * lda]` 的 lda 怎么乘到了 j 上。这是因为 BLAS 使用列优先顺序 (column-major order)。其实就是一列的元素连续储存在内存上，而不同行就需要用 lda 来索引。

在深度学习中使用的都是行优先顺序 (row-major order)，所以需要做一些调整。

> ps：如果用 CBLAS 接口可以直接指定行优先顺序，不需要下面的奇怪操作了。

显然，我们可以通过参数 transa, transb 来行列交换，这样矩阵 A, B 就不需要额外转置。

第二个问题，矩阵 C 也是列优先顺序，GEMM 不提供 transc 这样的参数，怎么办？答案是将 transa, transb 取反并交换 A, B，有公式：

$$\displaystyle \left(AB\right)^{\mathrm {T} }=B^{\mathrm {T} }A^{\mathrm {T} }$$

## 4. 数据类型

GEMM 支持 4 种数据类型：

1. sgemm: 单精度矩阵乘法 (s -> single)
2. dgemm: 双精度矩阵乘法 (d -> double)
3. cgemm: 单精度复数矩阵乘法
4. zgemm: 双精度复数矩阵乘法

显然满足不了深度学习的需求，于是就有了：

onednn

- gemm_bf16bf16f32: 输入 bfloat16 输出 float 的矩阵乘法
- gemm_s8u8s32, gemm_s8s8s32: 整数的矩阵乘法

cublas

- cublasSgemmEx: 数据类型作为参数输入，可支持 bfloat16, half, int8 等类型。[参考](https://docs.nvidia.com/cuda/cublas/#cublas-t-gemmex)

## 5. batch 扩展

BMM (Batched Matrix Multiplication) 是深度学习常见的算子，它可以同时进行多个规格相同的 GEMM 运算。我们希望在 bmm 接口内完成更合理的多线程策略来提升性能。

例如，cublas 提供了三个接口：

1. cublasgemmBatched，参数 A, B, C 被换成指针数组 Aarray, Barray, Carray，通过 `Aarray[i]` 得到第 i 个矩阵 A
2. cublasgemmStridedBatched，增加了 strideA, strideB, strideC，通过 `A + i * strideA` 得到第 i 个矩阵 A
3. cublasgemmGroupedBatched，每个 GEMM 的规格也可以不一样

## 6. pack 扩展

一般来说 GEMM 会先对数据布局进行一些调整，又叫 pack。

pack 可以让真正 GEMM 计算时访存变得友好，从而提升性能。很显然，这一步也可以放在外面做，这样进行多次 GEMM 计算就不需要再 pack 了。onednn [参考](https://www.intel.com/content/www/us/en/developer/articles/technical/introducing-the-new-packed-apis-for-gemm.html)

例如：线性层 (linear) 的权重矩阵在推理时不会改变，那就不需要每次 pack 了。

## 7. 算子融合

如果 GEMM 运算后面还要和 bias 向量做加法（如线性层中经常会出现 bias）。如果在 GEMM 里完成，就不需要反复读写矩阵 C 的内存。cublas [参考](https://docs.nvidia.com/cuda/cublas/#cublasltepilogue-t)

除了 bias，算子融合的方式有很多，例如压缩量化、激活函数等。
